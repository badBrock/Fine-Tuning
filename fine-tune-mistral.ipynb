{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:44:09.829860Z","iopub.execute_input":"2025-05-13T15:44:09.830559Z","iopub.status.idle":"2025-05-13T15:44:10.099273Z","shell.execute_reply.started":"2025-05-13T15:44:09.830533Z","shell.execute_reply":"2025-05-13T15:44:10.098532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install torch==2.5.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:44:14.471200Z","iopub.execute_input":"2025-05-13T15:44:14.472096Z","iopub.status.idle":"2025-05-13T15:47:28.248069Z","shell.execute_reply.started":"2025-05-13T15:44:14.472066Z","shell.execute_reply":"2025-05-13T15:47:28.247161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df=pd.read_csv(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:49:56.141499Z","iopub.execute_input":"2025-05-13T15:49:56.141812Z","iopub.status.idle":"2025-05-13T15:50:25.809680Z","shell.execute_reply.started":"2025-05-13T15:49:56.141783Z","shell.execute_reply":"2025-05-13T15:50:25.809141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:25.810665Z","iopub.execute_input":"2025-05-13T15:50:25.810905Z","iopub.status.idle":"2025-05-13T15:50:25.844947Z","shell.execute_reply.started":"2025-05-13T15:50:25.810886Z","shell.execute_reply":"2025-05-13T15:50:25.844417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.drop(\"id\" , axis = 1 ,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:25.845686Z","iopub.execute_input":"2025-05-13T15:50:25.845968Z","iopub.status.idle":"2025-05-13T15:50:25.879010Z","shell.execute_reply.started":"2025-05-13T15:50:25.845950Z","shell.execute_reply":"2025-05-13T15:50:25.878531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:29.897631Z","iopub.execute_input":"2025-05-13T15:50:29.897893Z","iopub.status.idle":"2025-05-13T15:50:30.001251Z","shell.execute_reply.started":"2025-05-13T15:50:29.897874Z","shell.execute_reply":"2025-05-13T15:50:30.000539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:31.855816Z","iopub.execute_input":"2025-05-13T15:50:31.856392Z","iopub.status.idle":"2025-05-13T15:50:31.861601Z","shell.execute_reply.started":"2025-05-13T15:50:31.856367Z","shell.execute_reply":"2025-05-13T15:50:31.861072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.iloc[0]['article']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:32.658014Z","iopub.execute_input":"2025-05-13T15:50:32.658506Z","iopub.status.idle":"2025-05-13T15:50:32.662764Z","shell.execute_reply.started":"2025-05-13T15:50:32.658486Z","shell.execute_reply":"2025-05-13T15:50:32.662259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"script = train_df.iloc[0]['article']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:34.606551Z","iopub.execute_input":"2025-05-13T15:50:34.607246Z","iopub.status.idle":"2025-05-13T15:50:34.610885Z","shell.execute_reply.started":"2025-05-13T15:50:34.607220Z","shell.execute_reply":"2025-05-13T15:50:34.610213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.iloc[0]['highlights']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:35.441687Z","iopub.execute_input":"2025-05-13T15:50:35.442314Z","iopub.status.idle":"2025-05-13T15:50:35.446712Z","shell.execute_reply.started":"2025-05-13T15:50:35.442291Z","shell.execute_reply":"2025-05-13T15:50:35.446136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = train_df.iloc[0]['highlights']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:36.148431Z","iopub.execute_input":"2025-05-13T15:50:36.148666Z","iopub.status.idle":"2025-05-13T15:50:36.152130Z","shell.execute_reply.started":"2025-05-13T15:50:36.148648Z","shell.execute_reply":"2025-05-13T15:50:36.151600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:36.818442Z","iopub.execute_input":"2025-05-13T15:50:36.818966Z","iopub.status.idle":"2025-05-13T15:50:36.823259Z","shell.execute_reply.started":"2025-05-13T15:50:36.818944Z","shell.execute_reply":"2025-05-13T15:50:36.822318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:37.865047Z","iopub.execute_input":"2025-05-13T15:50:37.865685Z","iopub.status.idle":"2025-05-13T15:50:38.517877Z","shell.execute_reply.started":"2025-05-13T15:50:37.865661Z","shell.execute_reply":"2025-05-13T15:50:38.517089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:45.408390Z","iopub.execute_input":"2025-05-13T15:50:45.408709Z","iopub.status.idle":"2025-05-13T15:50:45.415784Z","shell.execute_reply.started":"2025-05-13T15:50:45.408687Z","shell.execute_reply":"2025-05-13T15:50:45.415209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:50:48.445363Z","iopub.execute_input":"2025-05-13T15:50:48.445693Z","iopub.status.idle":"2025-05-13T15:52:31.674358Z","shell.execute_reply.started":"2025-05-13T15:50:48.445666Z","shell.execute_reply":"2025-05-13T15:52:31.673766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_target_modules(model, target_class_name=\"Linear4bit\", return_full_names=False):\n    \"\"\"\n    Find all unique module names in the model that match a given class name substring.\n\n    Args:\n        model (torch.nn.Module): The model to inspect.\n        target_class_name (str): Substring to look for in module types (e.g., \"Linear4bit\").\n        return_full_names (bool): If True, return full module names; otherwise, just the last part.\n\n    Returns:\n        List[str]: Unique module name parts where the target class was found.\n    \"\"\"\n    unique_layers = set()\n\n    for name, module in model.named_modules():\n        if target_class_name in type(module).__name__:\n            layer_name = name if return_full_names else name.split('.')[-1]\n            unique_layers.add(layer_name)\n\n    return sorted(unique_layers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:21.261475Z","iopub.execute_input":"2025-05-13T15:54:21.262587Z","iopub.status.idle":"2025-05-13T15:54:21.267273Z","shell.execute_reply.started":"2025-05-13T15:54:21.262561Z","shell.execute_reply":"2025-05-13T15:54:21.266577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res = find_target_modules(model, target_class_name=\"Linear4bit\", return_full_names=False)\nprint(res)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:23.299042Z","iopub.execute_input":"2025-05-13T15:54:23.299702Z","iopub.status.idle":"2025-05-13T15:54:23.304409Z","shell.execute_reply.started":"2025-05-13T15:54:23.299677Z","shell.execute_reply":"2025-05-13T15:54:23.303782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = res,\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:24.575249Z","iopub.execute_input":"2025-05-13T15:54:24.575732Z","iopub.status.idle":"2025-05-13T15:54:30.752098Z","shell.execute_reply.started":"2025-05-13T15:54:24.575712Z","shell.execute_reply":"2025-05-13T15:54:30.751307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset_df = train_df.sample(n=5000, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:30.901621Z","iopub.execute_input":"2025-05-13T15:54:30.901927Z","iopub.status.idle":"2025-05-13T15:54:30.926996Z","shell.execute_reply.started":"2025-05-13T15:54:30.901907Z","shell.execute_reply":"2025-05-13T15:54:30.926374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:32.336397Z","iopub.execute_input":"2025-05-13T15:54:32.336686Z","iopub.status.idle":"2025-05-13T15:54:32.345351Z","shell.execute_reply.started":"2025-05-13T15:54:32.336666Z","shell.execute_reply":"2025-05-13T15:54:32.344703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# 1. Define the Alpaca-style prompt template\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nSummarize the following text:\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\n# 2. Get the EOS token from your tokenizer\nEOS_TOKEN = tokenizer.eos_token\n\n# 3. Define the formatting function\ndef formatting_prompts_func(examples):\n    inputs = examples[\"article\"]     # raw input text\n    outputs = examples[\"highlights\"] # summary (fixed typo)\n    texts = [alpaca_prompt.format(inp, out) + EOS_TOKEN for inp, out in zip(inputs, outputs)]\n    return { \"text\": texts }\n\n# 4. Convert your DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(subset_df)\n\n# 5. Apply the formatting function\ndataset = dataset.map(formatting_prompts_func, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:37.420438Z","iopub.execute_input":"2025-05-13T15:54:37.420710Z","iopub.status.idle":"2025-05-13T15:54:37.946272Z","shell.execute_reply.started":"2025-05-13T15:54:37.420691Z","shell.execute_reply":"2025-05-13T15:54:37.945707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:39.752338Z","iopub.execute_input":"2025-05-13T15:54:39.752599Z","iopub.status.idle":"2025-05-13T15:54:39.757387Z","shell.execute_reply.started":"2025-05-13T15:54:39.752581Z","shell.execute_reply":"2025-05-13T15:54:39.756628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer for Unsloth's Mistral-7B 4bit model\ntokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-bnb-4bit\", use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:43.273982Z","iopub.execute_input":"2025-05-13T15:54:43.274274Z","iopub.status.idle":"2025-05-13T15:54:44.377214Z","shell.execute_reply.started":"2025-05-13T15:54:43.274252Z","shell.execute_reply":"2025-05-13T15:54:44.376517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True,  # Speeds up training for short text inputs\n    args = TrainingArguments(\n        per_device_train_batch_size = 4,       # Increased for speed\n        gradient_accumulation_steps = 1,       # Simpler, higher throughput\n        num_train_epochs = 1,                  # Faster experimentation\n        warmup_steps = 10,\n        learning_rate = 5e-5,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 5,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\"\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:54:49.287774Z","iopub.execute_input":"2025-05-13T15:54:49.288320Z","iopub.status.idle":"2025-05-13T15:54:58.675225Z","shell.execute_reply.started":"2025-05-13T15:54:49.288297Z","shell.execute_reply":"2025-05-13T15:54:58.674378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:55:02.067663Z","iopub.execute_input":"2025-05-13T15:55:02.068034Z","iopub.status.idle":"2025-05-13T15:55:02.073676Z","shell.execute_reply.started":"2025-05-13T15:55:02.067996Z","shell.execute_reply":"2025-05-13T15:55:02.073068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:55:04.833453Z","iopub.execute_input":"2025-05-13T15:55:04.833814Z","iopub.status.idle":"2025-05-13T22:38:56.639202Z","shell.execute_reply.started":"2025-05-13T15:55:04.833788Z","shell.execute_reply":"2025-05-13T22:38:56.638299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"script_2 = train_df.iloc[10]['article']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:40:49.808515Z","iopub.execute_input":"2025-05-13T22:40:49.809327Z","iopub.status.idle":"2025-05-13T22:40:49.813722Z","shell.execute_reply.started":"2025-05-13T22:40:49.809299Z","shell.execute_reply":"2025-05-13T22:40:49.813005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"script_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:40:58.456067Z","iopub.execute_input":"2025-05-13T22:40:58.456365Z","iopub.status.idle":"2025-05-13T22:40:58.461680Z","shell.execute_reply.started":"2025-05-13T22:40:58.456344Z","shell.execute_reply":"2025-05-13T22:40:58.461057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch, re\n\n# Enable fast inference mode\nFastLanguageModel.for_inference(model)\n\n# Add special tokens to ensure proper stopping\nif tokenizer.eos_token is None:\n    tokenizer.eos_token = \"</s>\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Ultra-simplified prompt with explicit end markers\nprompt = f\"\"\"<|im_start|>system\nYou are a summarization AI. Generate only a 2-3 sentence summary. Always complete your sentences.\n<|im_end|>\n<|im_start|>user\nSummarize this article in 2-3 complete sentences:\n\n{script_2}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\n# Add explicit ending tokens to the tokenizer's vocabulary if not present\nend_token = \"<|im_end|>\"\nif end_token not in tokenizer.get_vocab():\n    print(\"End token not in vocabulary, using EOS token instead\")\n    end_token = tokenizer.eos_token\n\n# Tokenize\ninputs = tokenizer(\n    [prompt],\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True,\n    max_length=2048,\n).to(\"cuda\")\n\n# Force the model to generate a complete summary\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    min_new_tokens=20,\n    temperature=0.1,  # Very low temperature to reduce randomness\n    top_p=0.5,       # More restrictive top_p\n    do_sample=False, # Turn off sampling for more deterministic output\n    num_beams=None,     # Use beam search for better completion\n    early_stopping=True,\n    repetition_penalty=1.0,  # Default repetition penalty\n    length_penalty=1.0,      # No length penalty\n    use_cache=True\n)\n\n# Get the generated text\nfull_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\ninput_text = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\nsummary = full_output[len(input_text):].strip()\n\n# Final cleanup to ensure we have only the summary\nsummary = re.sub(r'(?i)(summary:|the summary is:|here\\'s a summary:|step-by-step|instructions:|note:|###)', '', summary)\nsummary = re.sub(r'^\\d+\\.[\\s]*', '', summary, flags=re.MULTILINE)\nsummary = re.sub(r'^\\*[\\s]*', '', summary, flags=re.MULTILINE)\nsummary = re.sub(r'<\\|im_end\\|>.*', '', summary, flags=re.MULTILINE)\n\n\n\n\n, '', summary, flags=re.DOTALL)  # Remove anything after end token\n\n# Ensure the summary ends with proper punctuation\nif summary and not summary[-1] in ['.', '!', '?']:\n    # Find the last complete sentence\n    last_sentence_end = max(summary.rfind('.'), summary.rfind('!'), summary.rfind('?'))\n    if last_sentence_end > 0:\n        summary = summary[:last_sentence_end+1]\n\nprint(\"SUMMARY:\\n\", summary.strip())\n\n# Post-processing function to call if you still get incomplete summaries\ndef get_complete_sentences(text):\n    \"\"\"Extract only complete sentences from text\"\"\"\n    sentences = re.findall(r'[^.!?]*[.!?]', text)\n    if len(sentences) == 0:\n        return text  # Return original if no complete sentences found\n    return ' '.join(sentences)\n\n# Uncomment this line if you still get incomplete summaries\n# print(\"CLEANED SUMMARY:\\n\", get_complete_sentences(summary))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:41:27.891286Z","iopub.execute_input":"2025-05-13T22:41:27.891930Z","iopub.status.idle":"2025-05-13T22:41:43.758867Z","shell.execute_reply.started":"2025-05-13T22:41:27.891900Z","shell.execute_reply":"2025-05-13T22:41:43.757904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(\"badbrock/mistral-7b-finetuned\")\ntokenizer.push_to_hub(\"badbrock/mistral-7b-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:46:00.778424Z","iopub.execute_input":"2025-05-13T22:46:00.779156Z","iopub.status.idle":"2025-05-13T22:46:08.899909Z","shell.execute_reply.started":"2025-05-13T22:46:00.779129Z","shell.execute_reply":"2025-05-13T22:46:08.899215Z"}},"outputs":[],"execution_count":null}]}